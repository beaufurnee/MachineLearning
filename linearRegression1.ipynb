{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC ML LabExercise - Univariate Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement univariate linear regression. You will implement a gradient descent procedure to iteratively search for the solution. \n",
    "$$\n",
    "\\newcommand{\\ls}[1]{{}^{(#1)}}\n",
    "\\renewcommand{\\v}[1]{\\boldsymbol{#1}}\n",
    "\\renewcommand{\\T}{{}^T}\n",
    "\\newcommand{\\matvec}[1]{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data $(x\\ls 1,y\\ls 1),\\ldots,(x\\ls m, y\\ls m)$ where the $x$ values are the independent variables, these values are error free. The dependent values $y$ do contain errors.\n",
    "\n",
    "Linear regression fits a model function (*hypothesis*) $h_{\\v\\theta}(x)$ such that the sum of squared errors is minimized:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\v\\theta}(x\\ls i) - y\\ls i)^2\n",
    "$$\n",
    "Linear regression is called *linear* regression because we assume the hypothesis function $h_{\\v\\theta}$ is linear in its parameters $\\v\\theta$:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\theta_0 \\phi_0(x) + \\cdots + \\theta_n \\phi_n(x)\n",
    "$$\n",
    "where $\\phi_0,\\ldots,\\phi_n$ are arbitrary functions in $x$. In case we write:\n",
    "$$\n",
    "\\v x = \\matvec{\\phi_0(x)\\\\\\vdots\\\\\\phi_n(x)}\n",
    "$$\n",
    "the hypothesis function becomes:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\v\\theta\\T \\v x\n",
    "$$\n",
    "and the cost function is:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i)^2\n",
    "$$\n",
    "The gradient is given by:\n",
    "$$\n",
    "\\frac{\\partial J(\\v\\theta)}{\\partial \\v\\theta} =\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i) \\v x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in Practice I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple example. We will generate data with:\n",
    "$$\n",
    "   y\\ls i = a x\\ls i + b + R\n",
    "$$\n",
    "where $R$ is a random variable, i.e. its value is not exactly\n",
    "known. We assume here that $R$ is normally distributed with mean zero\n",
    "and standard deviation 0.3.\n",
    "\n",
    "We collect all values $x\\ls i$ for $i=1,\\ldots,m$ in an array of shape ``(m,)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEp1JREFUeJzt3W+IZfddx/HPx2xDm1gaNWOpSceNUCJpaJPuJbQGOkNi\nJbWh6SNpQalFGBCtqShSBdmJUPSBiH0gwrK2BlpTZJtiKWqNNbM1Dxq7m6xt86dYY6bZNOlu0DRa\nwVj9+mDuuGfv3nPnd+49v/P3/YJl786eOfd3SfjMd7/n98cRIQBAf3xf2wMAAFRDcANAzxDcANAz\nBDcA9AzBDQA9Q3ADQM8Q3ADQMwQ3APQMwQ0APXMox02vvvrqOHz4cI5bA8AgnT59+oWIWEu5Nktw\nHz58WKdOncpxawAYJNu7qdfSKgGAniG4AaBnCG4A6BmCGwB6huAGgJ4huAGgzPZ22yOYi+AGgDL3\n3NP2COYiuAGgLg1V6AQ3ABRtb0v23i/pwuuUUG6oQneOw4Ink0mwchJA79lSlYysev1F3+rTETFJ\nuZaKGwBWsUqFvqQse5UAwCAcPXrwNdvbF0J6hYq7CipuACjDdEAAGLiUCr0GBDcA1IXpgACAeQhu\nAOgZghsAeobgBoCeIbgBoGcIbgDoGYIbAHrmwOC2fb3tM4VfL9n+UBODA4DO6NAqygODOyK+HhE3\nRcRNko5I+k9Jn8k+MgDokg4dqlC1VXK7pH+OiN0cgwEAHKxqcL9X0n05BgIAndPClq0pkg9SsH25\npG9JemNEfHvO329J2pKk9fX1I7u7FOUABiTzlq25DlJ4p6RH5oW2JEXEsYiYRMRkbW2twm0BoCEd\nesC4iirB/T7RJgHQZ6s8YGxoy9YUScFt+0pJ75B0f97hAEALUirxDlXrScEdEd+NiB+KiO/kHhAA\n1KrsAePm5oVrOjTVLwUrJwEM2/b23kPF/QeL+69Pnmx1WKsguAGMVwen+qUguAGMx8bGxWFdVKzK\nO47gBjAeOzvz2yZFVfvdLVToBDcArDLVr4UHmwQ3gHGaDese9bsJbgDdUzUwlwnY4veUzTwpu+/m\nZqtBn7xXSRWTySROnTpV+30BjETVfUHq3Eck5V7Fa2p671x7lQBAvyxTAXdoaXsZghtAN1TdQjXl\n+mUeHFZ9v42N6u+xIlolALqnrP2wvT0/WMuuz7UVa4b70ioBMEwpFXRHDz+o06G2BwAAl6jaZy5e\nX6zKc1XcLffBqbgBdM/sVL2DKuimq+mWq3cqbgDdtkoF3YMZIsug4gYwXAPqaxcR3AD6Y6AVdFUE\nN4D+GGgFXRXBDQA9Q3ADQM8Q3ADQMwQ3gPrQg24EwQ2gPi2cBjNGBDeAfhpxdU9wA1hNXZs6LdpO\ndZ6y6n4Egc62rgDqs8qmTlW3Zm16K9fM2NYVwDCNYMvWFEnBbfsq2ydsP2n7Cdtvyz0wAD1UdUl6\n2aG7ZV+X5h/qO3vNwAM9qVVi+15Jfx8Rx21fLumKiHix7HpaJQCSpBy6S6vkEgdu62r7NZLeLunn\nJSkiXpb08ioDBICVjXjDqZRWyXWSzkv6uO1HbR+3fWXmcQEYqqqH7pYFdFkbZASBfmCrxPZE0pck\n3RoRD9v+qKSXIuK3Z67bkrQlSevr60d2d3czDRnAYPS0rZFD3bNKzko6GxEPT/98QtJbZi+KiGMR\nMYmIydraWvpoAQzTQB8MdsGBwR0Rz0t6xvb10y/dLunxrKMCUI+6FsEsI2X5+wjaGjmkziq5SdJx\nSZdLekrSByLi38quZ1YJ0BFVWxFl1xfPfcz13iNX+wKciDgzbYO8KSLesyi0AQxQ6uZRLJBpBCsn\ngaFJCc/Z16uE7ey95i2QKXtvLIW9SoAhW2Xxyvb2/Er76NGLw3ckC2RyY68SAOkW7cpXtrw8BQ8e\nsyG4gSErhmdZS6RYVaeE7T33VG/HlO09QttkKbRKgDFK2SOkqDirZPb6lO+v+n4jRKsEwMGqVMwp\n16MxBDcwRkePHjz7o9hCWTRbZNFeIlX2JEEyWiXA2DUxK4T2yIFolQBIl/IAs1iJM1ukdVTcAOar\ns0peZsn8yFBxA2PWxYDs4ph6jOAGhqZsX5Gq4UlLpLNolQBDw1LzXqJVAowNu/KNCsENDMGifUUI\n9MGhVQIMDa2SXqJVAowZDxUHj+AGcsjViki5b9k1BPpg0CoBcsjVlqjzTEh0Cq0SYGxSz4TEIBDc\nQF1yTckru+/m5mr3RW/RKgFyaKJVsh/ks2bPhEQvVGmVHMo9GAAZcarMKBHcQA65ZnBsbFxcaZdV\n3Rg0ghvIIVerYmfnwutilU1rZFR4OAkMAcE9KgQ30FcsqBmtpOC2/bTtr9o+Y5vpIhifHBXtohPV\nq34/RiVpOqDtpyVNIuKFlJsyHRCDk2PWxuzUPmaFjBorJwFgwFKDOyT9re3TtrdyDgjojBwrIcvu\nWdf9MQqprZJrIuJZ2z8s6QFJH4yIL85csyVpS5LW19eP7O7u5hgv0A5aJcis9lZJRDw7/f2cpM9I\numXONcciYhIRk7W1tSrjBdpVtcJtuiKmAseMA4Pb9pW2X73/WtJPSfpa7oEBjUnZWa849a7qKeop\n+2MvmtrHzn+YcWCrxPaPaa/KlvZWWv5ZRHxk0fcwqwS9UrVNUfVosFXbILRRRqHWVklEPBURb57+\neuNBoQ30QtUHj02eor5/KAIPLlGCbV2BVSru7e35rYyNDenkyUu/nrLl6ux4qLhHgXncwKycZ0BG\nXAjW/dc7O/O/TsWMGhDcGIdFD/iKDwZTgjXHHiGLWiPsSYIZtEowDqnthlXaEmUH9lY9yJfWyCjR\nKgGk5h/wVZ0OCCyJ4MZwlfWfZ4O0azM4aI3gAAQ3UBbwqd+bYzzAAgQ3xmGZKjZlxSKrGtECghvj\nkFrF0qZADxDcwKyD+t1d64ljdJgOiG6rOpWuTinT8qpO3Wvz86DTqkwHJLjRbW3Oac4R3MzRRgnm\ncQN1SOl30xNHCwhudE9Xesgp75d6TRc+DwaDVgm6bWithaF9HtSGVgmGiQoVkERwo+vKjgzra4jT\nE0cNaJWgP+o8FZ1peegYWiXormVOVJ/3YG9VLFVHjxHcaFbVwJzdAKpoP8Q3N+sYGdAbBDf6Zd4u\nfvPOdpyHaXkYCIIb+dUVmKs+2EvdnxvoOIIb+dUVmMXrNzaonjFah9oeALCUnZ0Lr5eZYcK0PPQY\nFTfyK1bBuQMzteKmMkePEdzIL2XhzCpBWrZIBxgoFuAgvxzbo+a+D9AwFuCgGYuq5Can3pW9F/O7\nMVDJFbftyySdkvRsRNy56Foq7pFIrW7Lrtvent/aOHp0+YCvc1k80KBcFffdkp5YbkjAHMyrBpaS\nFNy2r5X0LknH8w4HnbdMC6TqTJJVgpv53RiBpFaJ7ROSflfSqyX9+rxWie0tSVuStL6+fmR3d7fm\noSK7qjvm1dmKKL43DyoxQrW2SmzfKelcRJxedF1EHIuISURM1tbWEoeKTmlzKh0VMZAspVVyq6R3\n235a0qck3Wb7E1lHhX6oczFNjlkorI7EQFWax217UyWtkiJmlfRIjpkdVd573nvQ4sAIMY8b6XJs\nAJWKVY7AUioFd0TsHFRtY+DKArrOEKbFASxExT00OXrCxXuuGtApvWweVAILsVfJ0OToD5etRly1\nP04vG/h/9Lix2DIVbdmBvax8BBpHcA9B1al0Ke2O2XsW1RXQ9LKBpdAqGZocW6imbNxUddUlgIvQ\nKsGlch/YS2gDjeHMyaFZFKzL7gVSvCcBDbSOVskYMZsD6BxaJViMh4JArxHcY0S7A+g1gnvsCHGg\ndwjusWOjJ6B3CG4A6BmCu0/qamvkOLQAQGOYDtgnuTeQAtAapgP22VCq3qF8DqCDCO6umX1YmLut\nkWtONw89gWxolXTNotZFk22NVTeNogUDVEKrpG+aeFhY9V7LVMw89AQaQcVdVe7tSxdVqqu89ypb\nuTbxfsDIUXHn1GbvtuxcRqYJAqNCcHdN6sPC4g+Qsh8mm5vVgnh7u76jyNjICsiG4C5aFGh1VKKL\n7n/QNcs4ebK9MyGp0oFs6HEX5Tj2K+V7U+9Zdqp6UfGE9ZQjxxa9F+ELNIYe91CVtTJmX8/718HG\nRvX3AtBJBHfVNkjV3m3Z/av2n6u837xA39lZ7b4AOoNWSVHuKWyrtkqKiq2MsrYGU/KA3qi1VWL7\nlbb/wfY/2n7MNmuZuyDlgSYzO4BBSjnl/b8k3RYR/2H7FZIesv1XEfGlzGNrXu6gK7t/rvelTw0M\nUqVWie0rJD0k6Rcj4uGy63rbKgGAltQ+q8T2ZbbPSDon6YF5oW17y/Yp26fOnz9fbcRDRtULoGZJ\nwR0R/xMRN0m6VtIttm+cc82xiJhExGRtba3ucV6sT2HI9qYAalZpOmBEvCjpQUl35BlOorrCMNcP\ngD79YAHQOymzStZsXzV9/SpJ75D0ZO6BNSL1B8AyW6KyWROATFIq7tdJetD2VyR9WXs97s/lHdYc\nbe5ct0yF39YeIQAG78DgjoivRMTNEfGmiLgxIn6niYFdoq6d6+r8ATA7l3refQGgZuNb8p76AyAl\n4IuVeNl9WQQDoGb9XPJe1851qUvCqy5VZ6k5gIqGvztgmyecp1TiVNkAMupnxd00NnECkNnwK+6m\nMRsEQIcQ3KugJQKgBQT3KqjEAbSA4G4LoQ9gSeMI7lUX1+SQshqTcAcwxzhmlSwz+6OtY8yaHAOA\nzmBWSVe1ud8KgMEYbnAvE5K5gzVluT3hDuAAtErq/J6670+rBBgNWiV9wBxwAEvqf3CntBCKIZna\ncsgdrFXHDQBT/W+VVG0n5Go/1LVjIYBRolXSBg4FBtCQfgZ31ZkXzNQAMCDdDu5FQVzlGLO6jj2b\nd19+IABoWLd73DmmzOXqcTN1D8AKxtXjLpsxUlb1MlMDQM91L7iX6V/vKz4gLHtYmKuNwQ8EAA05\n1PYA5tpvOfSp/UBfG0BDuldxV51WV1ahF18TqgAGpJsV976U9kNx4UuxQu9TtQ4AFXSj4l5UNQMA\nLtKNirusaq6qWKHzsBDAQB1Ycdt+ve0HbT9u+zHbdzcxsEuk9KlTpgMCQM+ltEq+J+nXIuIGSW+V\n9Eu2b8g2orJKmb1AAEBSQnBHxHMR8cj09b9LekLSNdlGRKUMAAtVejhp+7CkmyU9nGMwl2AvEAC4\nRPJeJba/X9JJSR+JiPvn/P2WpC1JWl9fP7K7u1vnOJneB2DQat+rxPYrJH1a0ifnhbYkRcSxiJhE\nxGRtbS19tACASlJmlVjSn0h6IiL+IP+QSjC9DwAkpVXct0r6OUm32T4z/fXTmcd1KfraACApYQFO\nRDwkiWWMANAR3VjyDgBIRnADQM8Q3ADQMwQ3APRMlsOCbZ+XtOwKnKslvVDjcPqAzzx8Y/u8Ep+5\nqh+NiKRFMFmCexW2T6WuHhoKPvPwje3zSnzmnGiVAEDPENwA0DNdDO5jbQ+gBXzm4Rvb55X4zNl0\nrscNAFisixU3AGCBzgS37Ttsf932N2x/uO3x5NaZszxbYPsy24/a/lzbY2mC7atsn7D9pO0nbL+t\n7THlZvtXp/9ff832fbZf2faY6mb7Y7bP2f5a4Ws/aPsB2/80/f0Hcrx3J4Lb9mWS/kjSOyXdIOl9\nWc+17IZmz/Lslru1dwTeWHxU0l9HxI9LerMG/tltXyPpVyRNIuJGSZdJem+7o8riTyXdMfO1D0v6\nQkS8QdIXpn+uXSeCW9Itkr4REU9FxMuSPiXprpbHlFXjZ3l2hO1rJb1L0vG2x9IE26+R9Hbt7Wmv\niHg5Il5sd1SNOCTpVbYPSbpC0rdaHk/tIuKLkv515st3Sbp3+vpeSe/J8d5dCe5rJD1T+PNZjSDE\n9jV+lme7/lDSb0j637YH0pDrJJ2X9PFpe+i47SvbHlROEfGspN+X9E1Jz0n6TkT8TbujasxrI+K5\n6evnJb02x5t0JbhHa3qW56clfSgiXmp7PDnZvlPSuYg43fZYGnRI0lsk/XFE3Czpu8r0z+eumPZ1\n79LeD60fkXSl7Z9td1TNi70pe1mm7XUluJ+V9PrCn6+dfm3QUs7yHJhbJb3b9tPaa4fdZvsT7Q4p\nu7OSzkbE/r+mTmgvyIfsJyX9S0Scj4j/lnS/pJ9oeUxN+bbt10nS9PdzOd6kK8H9ZUlvsH2d7cu1\n9yDjsy2PKavOnOXZoIj4zYi4NiIOa++/8d9FxKArsYh4XtIztq+fful2SY+3OKQmfFPSW21fMf3/\n/HYN/IFswWclvX/6+v2S/iLHmxx4dFkTIuJ7tn9Z0ue19wT6YxHxWMvDym3/LM+v2j4z/dpvRcRf\ntjgm5PFBSZ+cFiVPSfpAy+PJKiIetn1C0iPamz31qAa4itL2fZI2JV1t+6yko5J+T9Kf2/4F7e2Q\n+jNZ3puVkwDQL11plQAAEhHcANAzBDcA9AzBDQA9Q3ADQM8Q3ADQMwQ3APQMwQ0APfN/hmHOR1A6\n1CEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ce75c0db70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 100;\n",
    "a = 0.5\n",
    "b = 2\n",
    "x = np.linspace(0,10,m)\n",
    "y = a * x + b + 0.3 * random.randn(m)\n",
    "plot(x, y, 'r+');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that $x\\ls i$ is stored in ``x[i]`` (and equivalently for $y$\n",
    "and ``y``).\n",
    "\n",
    "In this case we want to fit a model of the form $h_{\\v\\theta}(x)=a x + b$\n",
    "to the data. Note that with \n",
    "$$\n",
    "   \\v x = \\matvec{1\\\\x}\n",
    "$$\n",
    "(i.e. with $\\phi_0(x)=1$ and $\\phi_1(x)=x$) we have:\n",
    "$$\n",
    "   h_{\\v\\theta}(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "where $\\theta_0$ is $a$ and $\\theta_0$ is $b$. A constant function\n",
    "$\\phi_0$ in a linear hypothesis (*linear in its parameters!*) is often\n",
    "called a bias term of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``cost(theta, x, y)`` that calculates the cost. Note that ``x`` is the vector with all $x\\ls i$-values and ``y`` is the vector with all $y\\ls i$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045 0.0357064245709\n"
     ]
    }
   ],
   "source": [
    "def cost(theta0, theta1, x, y):\n",
    "    hypothesis = theta0 + theta1*x\n",
    "    result = (1/(2*x.size))*sum((hypothesis-y)**2)\n",
    "    return result\n",
    "\n",
    "print(0.3**2/2, cost(2, 0.5, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function called with ``cost(b,a,x,y)`` (where ``b``, ``a``,\n",
    "``x`` and ``y``) are defined as in the previous code snippet,\n",
    "should a return a value that is close to $0.3^2/2$ (For extra\n",
    "points: can you prove this?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``theta0, theta1 = gradDescentStep(theta0, theta1, x, y)`` that does the calculations for one gradient descent step. In this function we use the Python possibility to return a tuple of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescentStep(learningrate, theta0, theta1, x, y):\n",
    "    #your code here\n",
    "    return theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with values ``theta0 = theta1 = 0``. Calculate the costfor these values. After the gradient descent step, using ``learningrate=0.01``, resulting in new theta values again calculate the cost. If all went well the cost should have decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta0 = theta1 = 0\n",
    "costbefore = cost(theta0, theta1, x, y)\n",
    "theta0, theta1 = gradDescentStep(0.01, theta0, theta1, x, y)\n",
    "costafter = cost(theta0, theta1, x, y)\n",
    "print(costbefore, '>=', costafter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Problem: Implement Least Squares with closed form solution\n",
    "\n",
    "For the Least Squares method there is also a closed-form solution.\n",
    "\n",
    "$\\theta_1$ can be found by:\n",
    "$$ \\boldsymbol{\\hat\\theta_1} =( X ^TX)^{-1}X^{T}\\boldsymbol y $$\n",
    "\n",
    "You can leave $\\theta_0$ to be 0. Make a plot with your data as dots and your prediction as a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
